{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/wandb/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/wandb/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/wandb/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/wandb/\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/wandb/\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement wandb==0.8.22 (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for wandb==0.8.22\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb==0.8.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wandb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-74dbda0aee27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhw_grapheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mixup\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate_stratified_k_fold_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhw_grapheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhw_grapheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_pipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/hugo/kaggle_bengali_recognition/hw_grapheme/train_mixup.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhw_grapheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoss_combine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutmix_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixup_criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m##### for mix up training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wandb'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from hw_grapheme.train_mixup import generate_stratified_k_fold_index, train_model\n",
    "from hw_grapheme.utils import load_model_weight\n",
    "from hw_grapheme.data_pipeline import create_dataloaders, load_data\n",
    "from hw_grapheme.model import EfficientNet_grapheme, EfficientNet_0\n",
    "from hw_grapheme.loss_func import Loss_combine\n",
    "\n",
    "# from torchtools.optim import RangerLars, RAdam\n",
    "# from one_cycle import OneCycleLR\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "# from warmup_scheduler import GradualWarmupScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "#     mean = np.array([0.485, 0.456, 0.406])\n",
    "#     std = np.array([0.229, 0.224, 0.225])\n",
    "#     inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "pickle_paths = [\n",
    "    \"../data/processed_data/size_224/train_data_0.pickle\",\n",
    "#     \"../data/processed_data/size_224/train_data_1.pickle\",\n",
    "#     \"../data/processed_data/size_224/train_data_2.pickle\",\n",
    "#     \"../data/processed_data/size_224/train_data_3.pickle\",\n",
    "]\n",
    "\n",
    "image_data, name_data, label_data = load_data(pickle_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"model\": \"efficient 0\",\n",
    "    \"pretrain\": False,\n",
    "    \"head_info\": \"1 fc\",\n",
    "    \"input_size\": \"224X224\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"image_processing\": \"mixup, cutmix\",\n",
    "    \"cutmix/mixup alpha\": 0.1,\n",
    "    'mixup_alpha' : 0.1,\n",
    "    'batch_size': 64,\n",
    "    'num_workers': 6,\n",
    "    'pin_memory': True,\n",
    "    'n_epoch': 120,\n",
    "    'n_splits': 5,\n",
    "    'random_seed': 2020,\n",
    "    'mix_precision': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_stratified_k_fold_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ded407f37b18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmixed_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mix_precision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m train_idx_list, test_idx_list = generate_stratified_k_fold_index(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_stratified_k_fold_index' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = configs['batch_size']\n",
    "num_workers = configs['num_workers'] \n",
    "pin_memory = configs['pin_memory']\n",
    "n_epoch = configs['n_epoch']\n",
    "n_splits = configs['n_splits']\n",
    "random_seed = configs['random_seed']\n",
    "mixed_precision = configs['mix_precision']\n",
    "\n",
    "train_idx_list, test_idx_list = generate_stratified_k_fold_index(\n",
    "    image_data, label_data, n_splits, random_seed\n",
    ")\n",
    "\n",
    "# create loss function\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = Loss_combine()\n",
    "\n",
    "# for discriminative lr\n",
    "# my_list = ['module._fc.weight', 'module._fc.bias']\n",
    "# params = list(filter(lambda kv: kv[0] in my_list, eff_b0.named_parameters()))\n",
    "# base_params = list(filter(lambda kv: kv[0] not in my_list, eff_b0.named_parameters()))\n",
    "# params = [kv[1] for kv in params]\n",
    "# base_params = [kv[1] for kv in base_params]\n",
    "\n",
    "# create data_transforms\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # transforms.RandomAffine(degrees=10, scale=(1.0, 1.15)),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.0692], [0.2051]),\n",
    "        # transforms.ToPILImage(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.0692], [0.2051])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "# os.environ['WANDB_NOTEBOOK_NAME'] = 'model_3_efficient_net'\n",
    "%env WANDB_NOTEBOOK_NAME=model_3_efficient_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain = configs['pretrain']\n",
    "# eff_version = \"4\"\n",
    "mixup_alpha = configs['mixup_alpha']\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(zip(train_idx_list, test_idx_list)):\n",
    "    if i != 0:\n",
    "        continue\n",
    "    print(f\"Training fold {i}\")\n",
    "    MODEL_DIR = Path(f\"../model_weights/eff_0_with_mixup_cutmix_alpha_0.1/fold_{i}\")\n",
    "    MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    # WandB â€“ Initialize a new run\n",
    "    # use your own user/project name\n",
    "    wandb.init(entity=\"noklam\", project='bengali',config=configs, group=current_time)\n",
    "    # Re-run the model without restarting the runtime, unnecessary after our next release\n",
    "    wandb.watch_called = False\n",
    "\n",
    "    \n",
    "    # create model\n",
    "    # eff_b0 = EfficientNet_grapheme(eff_version, pretrain)\n",
    "    eff_b0 = EfficientNet_0(pretrain)\n",
    "\n",
    "    #########################\n",
    "    # load_model_weight(eff_b0, \"../model_weights/eff_0_with_mixup_cutmix/fold_0/eff_0_low_loss.pth\")\n",
    "    #########################\n",
    "    if mixed_precision:\n",
    "        eff_b0 = apex.parallel.DistributedDataParallel(eff_b0)\n",
    "        eff_b0.to(\"cuda\")\n",
    "        eff_b0 = torch.nn.parallel.DistributedDataParallel(\n",
    "            eff_b0, device_ids=[0, 1], output_device=0\n",
    "        )\n",
    "        eff_b0, optimizer_ft = amp.initialize(\n",
    "            eff_b0, optimizer_ft, opt_level=\"O1\")\n",
    "    else:\n",
    "        eff_b0.to(\"cuda\")\n",
    "        eff_b0 = nn.DataParallel(eff_b0)\n",
    "        # Add W&B logging\n",
    "        wandb.watch(eff_b0, log='all')\n",
    "\n",
    "    # create optimizer\n",
    "    # optimizer_ft = RangerLars(eff_b0.parameters())\n",
    "    optimizer_ft = optim.Adam(eff_b0.parameters(), weight_decay=1e-5)\n",
    "\n",
    "    # create data loader\n",
    "    data_loaders = create_dataloaders(\n",
    "        image_data, name_data, label_data, train_idx, valid_idx,\n",
    "        data_transforms, batch_size, num_workers, pin_memory\n",
    "    )\n",
    "\n",
    "    # create lr scheduler\n",
    "    # exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    #     optimizer_ft, factor=0.5, patience=5,\n",
    "    # )\n",
    "#     cos_lr_scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "#         optimizer_ft, T_max=n_epoch,\n",
    "#     )\n",
    "#     # one_cycle_lr_scheduler = OneCycleLR(\n",
    "#     #     optimizer_ft, max_lr=0.01, steps_per_epoch=len(data_loaders[\"train\"]), epochs=n_epoch\n",
    "#     # )\n",
    "\n",
    "#     scheduler_warmup = GradualWarmupScheduler(\n",
    "#         optimizer_ft, multiplier=1, total_epoch=10, after_scheduler=cos_lr_scheduler\n",
    "#     )\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    callbacks = {}\n",
    "\n",
    "    callbacks = train_model(\n",
    "        eff_b0, optimizer_ft, data_loaders,\n",
    "        mixed_precision, callbacks, mixup_alpha, num_epochs=n_epoch,\n",
    "        epoch_scheduler=None, save_dir=MODEL_DIR\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
